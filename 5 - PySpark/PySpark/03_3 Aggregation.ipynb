{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d7cdae-5127-46f9-b804-258fab3abd64",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "    1. Select\n",
    "    2. expr, Col\n",
    "    3. SelectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab2c2a9-7758-4d4d-9a86-619e25a7322f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .appName(\"Create Dataframe\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd7bbd9-efb2-4ad4-8ce0-ebeee4dc9264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ca6ed3-5991-47c8-9f30-a769fe932396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ba03929a6386:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fed87e27490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b6a22a-c597-4738-a5f5-40042028451b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5231afeb-922c-4c11-8e33-ecea22828e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sales_data = [\n",
    "['SO101', '08-01-2024', 'Amit Joshi', 'Hyderabad', 'India', 'Keyboard', 3000, 4, 'pieces', 12000, 'INR', '18-01-2024'],\n",
    "['SO102', '19-01-2024', 'Divya Reddy', 'Mumbai', 'India', 'Printer', 1500, 5, 'pieces', 7500, 'INR', '29-01-2024'],\n",
    "['SO103', '01-01-2024', 'Rohan Patel', 'Pune', 'India', 'Laptop', 1500, 10, 'pieces', 15000, 'INR', '11-01-2024'],\n",
    "['SO104', '30-03-2024', 'Divya Reddy', 'Pune', 'India', 'External Hard Drive', 10000, 8, 'pieces', 80000, 'INR', '09-04-2024'],\n",
    "['SO105', '14-03-2024', 'Nikhil Desai', 'Ahmedabad', 'India', 'Wireless Mouse', 8000, 10, 'pieces', 80000, 'INR', '19-03-2024'],\n",
    "['SO106', '20-03-2024', 'Pooja Mehta', 'Hyderabad', 'India', 'Monitor', 8000, 10, 'pieces', 80000, 'INR', '25-03-2024'],\n",
    "['SO107', '10-03-2024', 'Amit Joshi', 'Mumbai', 'India', 'Webcam', 10000, 2, 'pieces', 20000, 'INR', '15-03-2024'],\n",
    "['SO108', '03-04-2024', 'Nikhil Desai', 'Pune', 'India', 'Wireless Mouse', 8000, 1, 'pieces', 8000, 'INR', '08-04-2024'],\n",
    "['SO109', '03-05-2024', 'Amit Joshi', 'Ahmedabad', 'India', 'Monitor', 70000, 5, 'pieces', 350000, 'INR', '18-05-2024'],\n",
    "['SO1010', '01-05-2024', 'Nikhil Desai', 'Ahmedabad', 'India', 'Monitor', 2000, 7, 'pieces', 14000, 'INR', '16-05-2024'],\n",
    "['SO1011', '19-06-2024', 'Amit Joshi', 'Chennai', 'India', 'Printer', 2000, 1, 'pieces', 2000, 'INR', '04-07-2024'],\n",
    "['SO1012', '08-07-2024', 'Rohan Patel', 'Mumbai', 'India', 'Keyboard', 3000, 5, 'pieces', 15000, 'INR', '23-07-2024'],\n",
    "['SO1013', '14-08-2024', 'Nikhil Desai', 'Mumbai', 'India', 'Keyboard', 10000, 10, 'pieces', 100000, 'INR', '29-08-2024'],\n",
    "['SO1014', '02-09-2024', 'Pooja Mehta', 'Ahmedabad', 'India', 'Laptop', 10000, 10, 'pieces', 100000, 'INR', '17-09-2024'],\n",
    "['SO1015', '23-09-2024', 'Amit Joshi', 'Chennai', 'India', 'Laptop', 8000, 6, 'pieces', 48000, 'INR', '08-10-2024'],\n",
    "['SO1016', '20-01-2023', 'Nikhil Desai', 'Ahmedabad', 'India', 'Webcam', 8000, 4, 'pieces', 32000, 'INR', '09-02-2023'],\n",
    "['SO1017', '01-01-2023', 'Nikhil Desai', 'Mumbai', 'India', 'Keyboard', 8000, 6, 'pieces', 48000, 'INR', '21-01-2023'],\n",
    "['SO1018', '26-01-2023', 'Nikhil Desai', 'Pune', 'India', 'Laptop', 10000, 5, 'pieces', 50000, 'INR', '15-02-2023'],\n",
    "['SO1019', '18-02-2023', 'Rohan Patel', 'Mumbai', 'India', 'Keyboard', 2000, 5, 'pieces', 10000, 'INR', '10-03-2023'],\n",
    "['SO1020', '24-03-2023', 'Amit Joshi', 'Pune', 'India', 'External Hard Drive', 5000, 5, 'pieces', 25000, 'INR', '13-04-2023'],\n",
    "['SO1021', '16-03-2023', 'Amit Joshi', 'Mumbai', 'India', 'Webcam', 10000, 7, 'pieces', 70000, 'INR', '10-04-2023'],\n",
    "['SO1022', '30-04-2023', 'Amit Joshi', 'Pune', 'India', 'Laptop', 70000, 5, 'pieces', 350000, 'INR', '25-05-2023'],\n",
    "['SO1023', '13-04-2023', 'Nikhil Desai', 'Mumbai', 'India', 'Wireless Mouse', 3000, 3, 'pieces', 9000, 'INR', '08-05-2023'],\n",
    "['SO1024', '08-07-2023', 'Pooja Mehta', 'Ahmedabad', 'India', 'Wireless Mouse', 5000, 2, 'pieces', 10000, 'INR', '02-08-2023'],\n",
    "['SO1025', '25-07-2023', 'Rohan Patel', 'Chennai', 'India', 'External Hard Drive', 5000, 7, 'pieces', 35000, 'INR', '19-08-2023'],\n",
    "['SO1026', '14-07-2023', 'Divya Reddy', 'Ahmedabad', 'India', 'Keyboard', 1500, 8, 'pieces', 12000, 'INR', '08-08-2023'],\n",
    "['SO1027', '24-07-2023', 'Rohan Patel', 'Hyderabad', 'India', 'Laptop', 3000, 9, 'pieces', 27000, 'INR', '18-08-2023'],\n",
    "['SO1028', '25-07-2023', 'Rohan Patel', 'Chennai', 'India', 'Monitor', 5000, 8, 'pieces', 40000, 'INR', '19-08-2023'],\n",
    "['SO1029', '17-09-2023', 'Divya Reddy', 'Chennai', 'India', 'Wireless Mouse', 1500, 8, 'pieces', 12000, 'INR', '12-10-2023'],\n",
    "['SO1030', '08-09-2023', 'Nikhil Desai', 'Chennai', 'India', 'Webcam', 8000, 9, 'pieces', 72000, 'INR', '03-10-2023']\n",
    "\n",
    "]\n",
    "\n",
    "sales_schema = \"\"\"SalesOrder string, \n",
    "                    OrderDate string, \n",
    "                    CustomerName string, \n",
    "                    City string, \n",
    "                    Country string, \n",
    "                    Product string, \n",
    "                    Price integer, \n",
    "                    Qty_Sold integer, \n",
    "                    Qty_Sold_Units string, \n",
    "                    Amount integer, \n",
    "                    Amount_Currency string, \n",
    "                    ShipDate string\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9647546a-790d-4304-83b4-d106965b308c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(data=sales_data, schema=sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d73714ee-dc9a-4a44-bf30-ce6a3e52a9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrder: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- CustomerName: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Qty_Sold: integer (nullable = true)\n",
      " |-- Qty_Sold_Units: string (nullable = true)\n",
      " |-- Amount: integer (nullable = true)\n",
      " |-- Amount_Currency: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f55297-7f4a-4e58-9ce3-95b375059b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+---------+-------+-------------------+-----+--------+--------------+------+---------------+----------+\n",
      "|SalesOrder| OrderDate|CustomerName|     City|Country|            Product|Price|Qty_Sold|Qty_Sold_Units|Amount|Amount_Currency|  ShipDate|\n",
      "+----------+----------+------------+---------+-------+-------------------+-----+--------+--------------+------+---------------+----------+\n",
      "|     SO101|08-01-2024|  Amit Joshi|Hyderabad|  India|           Keyboard| 3000|       4|        pieces| 12000|            INR|18-01-2024|\n",
      "|     SO102|19-01-2024| Divya Reddy|   Mumbai|  India|            Printer| 1500|       5|        pieces|  7500|            INR|29-01-2024|\n",
      "|     SO103|01-01-2024| Rohan Patel|     Pune|  India|             Laptop| 1500|      10|        pieces| 15000|            INR|11-01-2024|\n",
      "|     SO104|30-03-2024| Divya Reddy|     Pune|  India|External Hard Drive|10000|       8|        pieces| 80000|            INR|09-04-2024|\n",
      "|     SO105|14-03-2024|Nikhil Desai|Ahmedabad|  India|     Wireless Mouse| 8000|      10|        pieces| 80000|            INR|19-03-2024|\n",
      "|     SO106|20-03-2024| Pooja Mehta|Hyderabad|  India|            Monitor| 8000|      10|        pieces| 80000|            INR|25-03-2024|\n",
      "|     SO107|10-03-2024|  Amit Joshi|   Mumbai|  India|             Webcam|10000|       2|        pieces| 20000|            INR|15-03-2024|\n",
      "|     SO108|03-04-2024|Nikhil Desai|     Pune|  India|     Wireless Mouse| 8000|       1|        pieces|  8000|            INR|08-04-2024|\n",
      "|     SO109|03-05-2024|  Amit Joshi|Ahmedabad|  India|            Monitor|70000|       5|        pieces|350000|            INR|18-05-2024|\n",
      "|    SO1010|01-05-2024|Nikhil Desai|Ahmedabad|  India|            Monitor| 2000|       7|        pieces| 14000|            INR|16-05-2024|\n",
      "|    SO1011|19-06-2024|  Amit Joshi|  Chennai|  India|            Printer| 2000|       1|        pieces|  2000|            INR|04-07-2024|\n",
      "|    SO1012|08-07-2024| Rohan Patel|   Mumbai|  India|           Keyboard| 3000|       5|        pieces| 15000|            INR|23-07-2024|\n",
      "|    SO1013|14-08-2024|Nikhil Desai|   Mumbai|  India|           Keyboard|10000|      10|        pieces|100000|            INR|29-08-2024|\n",
      "|    SO1014|02-09-2024| Pooja Mehta|Ahmedabad|  India|             Laptop|10000|      10|        pieces|100000|            INR|17-09-2024|\n",
      "|    SO1015|23-09-2024|  Amit Joshi|  Chennai|  India|             Laptop| 8000|       6|        pieces| 48000|            INR|08-10-2024|\n",
      "|    SO1016|20-01-2023|Nikhil Desai|Ahmedabad|  India|             Webcam| 8000|       4|        pieces| 32000|            INR|09-02-2023|\n",
      "|    SO1017|01-01-2023|Nikhil Desai|   Mumbai|  India|           Keyboard| 8000|       6|        pieces| 48000|            INR|21-01-2023|\n",
      "|    SO1018|26-01-2023|Nikhil Desai|     Pune|  India|             Laptop|10000|       5|        pieces| 50000|            INR|15-02-2023|\n",
      "|    SO1019|18-02-2023| Rohan Patel|   Mumbai|  India|           Keyboard| 2000|       5|        pieces| 10000|            INR|10-03-2023|\n",
      "|    SO1020|24-03-2023|  Amit Joshi|     Pune|  India|External Hard Drive| 5000|       5|        pieces| 25000|            INR|13-04-2023|\n",
      "+----------+----------+------------+---------+-------+-------------------+-----+--------+--------------+------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a20966-7a90-4a92-b303-2c6fa43162d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|CustomerName|max(Amount)|\n",
      "+------------+-----------+\n",
      "| Rohan Patel|      40000|\n",
      "|  Amit Joshi|     350000|\n",
      "| Divya Reddy|      80000|\n",
      "|Nikhil Desai|     100000|\n",
      "| Pooja Mehta|     100000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select CustomerName, sum(Amount) \n",
    "# from table \n",
    "# group by CustomerName\n",
    "from pyspark.sql.functions import col, sum, max\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).show()\n",
    "sales_df.groupBy(\"CustomerName\") \\\n",
    "        .agg(max(\"Amount\")) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6092c536-b81c-41c1-b283-61924331fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|CustomerName|total_amount|\n",
      "+------------+------------+\n",
      "| Rohan Patel|      142000|\n",
      "|  Amit Joshi|      877000|\n",
      "| Divya Reddy|      111500|\n",
      "|Nikhil Desai|      413000|\n",
      "| Pooja Mehta|      190000|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select CustomerName, sum(Amount) as total_amount\n",
    "# from table \n",
    "# group by customer\n",
    "from pyspark.sql.functions import col, sum\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).show()\n",
    "sales_df.groupBy(\"CustomerName\") \\\n",
    "        .agg(sum(\"Amount\").alias(\"total_amount\")) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d809952-0e01-4edd-acfe-304f7bb0f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|     City|CustomerName|sum(Amount)|\n",
      "+---------+------------+-----------+\n",
      "|     Pune| Rohan Patel|      15000|\n",
      "|Hyderabad|  Amit Joshi|      12000|\n",
      "|   Mumbai| Divya Reddy|       7500|\n",
      "|Ahmedabad|Nikhil Desai|     126000|\n",
      "|Hyderabad| Pooja Mehta|      80000|\n",
      "|     Pune| Divya Reddy|      80000|\n",
      "|Ahmedabad|  Amit Joshi|     350000|\n",
      "|   Mumbai|  Amit Joshi|      90000|\n",
      "|     Pune|Nikhil Desai|      58000|\n",
      "|  Chennai|  Amit Joshi|      50000|\n",
      "|   Mumbai| Rohan Patel|      25000|\n",
      "|   Mumbai|Nikhil Desai|     157000|\n",
      "|Ahmedabad| Pooja Mehta|     110000|\n",
      "|     Pune|  Amit Joshi|     375000|\n",
      "|Hyderabad| Rohan Patel|      27000|\n",
      "|  Chennai| Divya Reddy|      12000|\n",
      "|  Chennai|Nikhil Desai|      72000|\n",
      "|Ahmedabad| Divya Reddy|      12000|\n",
      "|  Chennai| Rohan Patel|      75000|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select CustomerName, City,sum(Amount) \n",
    "# from table \n",
    "# group by customer \n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).shaow()\n",
    "(sales_df.groupBy(\"City\",\"CustomerName\") \n",
    "        .agg(sum(\"Amount\")) \n",
    "        .show() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e47f40f-d8f6-471f-a955-75e17216d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------+\n",
      "|     City|CustomerName|total_amount|\n",
      "+---------+------------+------------+\n",
      "|   Mumbai| Divya Reddy|        7500|\n",
      "|Hyderabad|  Amit Joshi|       12000|\n",
      "|  Chennai| Divya Reddy|       12000|\n",
      "|Ahmedabad| Divya Reddy|       12000|\n",
      "|     Pune| Rohan Patel|       15000|\n",
      "|   Mumbai| Rohan Patel|       25000|\n",
      "|Hyderabad| Rohan Patel|       27000|\n",
      "|  Chennai|  Amit Joshi|       50000|\n",
      "|     Pune|Nikhil Desai|       58000|\n",
      "|  Chennai|Nikhil Desai|       72000|\n",
      "|  Chennai| Rohan Patel|       75000|\n",
      "|Hyderabad| Pooja Mehta|       80000|\n",
      "|     Pune| Divya Reddy|       80000|\n",
      "|   Mumbai|  Amit Joshi|       90000|\n",
      "|Ahmedabad| Pooja Mehta|      110000|\n",
      "|Ahmedabad|Nikhil Desai|      126000|\n",
      "|   Mumbai|Nikhil Desai|      157000|\n",
      "|Ahmedabad|  Amit Joshi|      350000|\n",
      "|     Pune|  Amit Joshi|      375000|\n",
      "+---------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select CustomerName, City,sum(Amount) \n",
    "# from table \n",
    "# group by customer \n",
    "# order by City\n",
    "from pyspark.sql.functions import col, sum\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).shaow()\n",
    "sales_df.groupBy(\"City\",\"CustomerName\") \\\n",
    "        .agg(sum(\"Amount\").alias(\"total_amount\")) \\\n",
    "        .orderBy(\"total_amount\") \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f9c65e-364f-429d-b32a-542395389dc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, \u001b[38;5;28msum\u001b[39m, desc\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).shaow()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sales_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_amount > 100000\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m()) \\\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "# select CustomerName, City,sum(Amount) \n",
    "# from table \n",
    "# group by customer \n",
    "# having sum(Amount) > 100000\n",
    "# order by City\n",
    "from pyspark.sql.functions import col, sum, desc\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).shaow()\n",
    "sales_df.groupBy(\"City\",\"CustomerName\") \\\n",
    "        .agg(sum(\"Amount\").alias(\"total_amount\")) \\\n",
    "        .where(\"total_amount > 100000\") \\\n",
    "        .orderBy(\"City\".desc()) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a844eb-2246-4a09-929a-ff942db0be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(total_amount#206L) AND (total_amount#206L > 100000))\n",
      "   +- HashAggregate(keys=[City#3, CustomerName#2], functions=[sum(Amount#9)])\n",
      "      +- Exchange hashpartitioning(City#3, CustomerName#2, 200), ENSURE_REQUIREMENTS, [plan_id=211]\n",
      "         +- HashAggregate(keys=[City#3, CustomerName#2], functions=[partial_sum(Amount#9)])\n",
      "            +- Project [CustomerName#2, City#3, Amount#9]\n",
      "               +- Scan ExistingRDD[SalesOrder#0,OrderDate#1,CustomerName#2,City#3,Country#4,Product#5,Price#6,Qty_Sold#7,Qty_Sold_Units#8,Amount#9,Amount_Currency#10,ShipDate#11]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select CustomerName, City,sum(Amount) \n",
    "# from table \n",
    "# group by customer \n",
    "# having sum(Amount) > 100000\n",
    "# order by City\n",
    "from pyspark.sql.functions import col, sum, desc\n",
    "# sales_df.groupby(\"customer\").agg(sum(\"Amount\")).shaow()\n",
    "sales_df.orderBy(col(\"City\").desc()) \\\n",
    "        .groupBy(\"City\",\"CustomerName\") \\\n",
    "        .agg(sum(\"Amount\").alias(\"total_amount\")) \\\n",
    "        .where(\"total_amount > 100000\") \\\n",
    "        .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc4a736-45a3-4bc4-852f-a12e73aa1955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627dd43c-6c86-4d0f-857e-c9ec791ed5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
